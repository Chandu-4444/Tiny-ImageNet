{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3GDHKn1NXdH",
        "outputId": "a74c991c-359a-47d0-81d9-f489a563233b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-03 03:02:13--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  21.2MB/s    in 26s     \n",
            "\n",
            "2021-12-03 03:02:40 (8.97 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM2OFhWSLTCa"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtaoTVi54fLd",
        "outputId": "41a3ad82-cbed-4ec2-b4ac-4da51bbb17f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on the CPU\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# if not os.path.exists(\"tiny-imagenet-200/\"):\n",
        "#     !wget http: // cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "#     !unzip - q tiny-imagenet-200.zip\n",
        "#     print(\"Data downloaded!\")\n",
        "# else:\n",
        "#     print(\"Data downloaded!\")\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "# from utils import helper_plot\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # you can continue going on here, like cuda:1 cuda:2....etc.\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")\n",
        "\n",
        "# Constants\n",
        "DATA_DIR = \"tiny-imagenet-200\"\n",
        "VAL_DIR = \"tiny-imagenet-200/val\"\n",
        "VAL_IMG_DIR = \"tiny-imagenet-200/val/images\"\n",
        "\n",
        "train_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomRotation(30),\n",
        "    torchvision.transforms.Resize(255),\n",
        "    torchvision.transforms.RandomResizedCrop(224),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomVerticalFlip(),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize(255),\n",
        "    torchvision.transforms.CenterCrop(224),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(\n",
        "    \"tiny-imagenet-200/train\", transform=train_transforms)\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
        "# helper_plot.plot_grid(dataloader=train_dataloader)\n",
        "\n",
        "# Place corresponding images into respective folders\n",
        "val_info = pd.read_csv(\"tiny-imagenet-200/val/val_annotations.txt\",\n",
        "                       sep='\\t', header=None, names=[\"File\", \"Class\", \"X\", \"Y\", \"H\", \"W\"])\n",
        "val_info.drop([\"X\", \"Y\", \"H\", \"W\"], axis=1, inplace=True)\n",
        "for img, folder_name in zip(val_info[\"File\"], val_info[\"Class\"]):\n",
        "    newpath = os.path.join(VAL_IMG_DIR, folder_name)\n",
        "    if not os.path.exists(newpath):\n",
        "        os.makedirs(newpath)\n",
        "    if os.path.exists(os.path.join(VAL_IMG_DIR, img)):\n",
        "        os.rename(os.path.join(VAL_IMG_DIR, img), os.path.join(newpath, img))\n",
        "\n",
        "val_dataset = torchvision.datasets.ImageFolder(\n",
        "    \"tiny-imagenet-200/val/images\", transform=test_transforms)\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "# helper_plot.plot_grid(dataloader=val_dataloader)\n",
        "\n",
        "\n",
        "# class AlexNet(nn.Module):\n",
        "#     def __init__(self, num_classes: int = 200, dropout: float = 0.5) -> None:\n",
        "#         super().__init__()\n",
        "#         self.features = nn.Sequential(\n",
        "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool2d(kernel_size=2),\n",
        "#             nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "#             nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "#         )\n",
        "#         self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Dropout(p=dropout),\n",
        "#             nn.Linear(256 * 6 * 6, 4096),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Dropout(p=dropout),\n",
        "#             nn.Linear(4096, 4096),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Linear(4096, num_classes),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "#         x = self.features(x)\n",
        "#         x = self.avgpool(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.classifier(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# model = AlexNet(num_classes=200).to(device)\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimiser = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "#     optimizer=optimiser, step_size=5, gamma=0.99)\n",
        "\n",
        "# # from utils import misc\n",
        "\n",
        "# # for idx, (image, label) in enumerate(train_dataloader):\n",
        "# #     print(idx, image.shape, label.shape)\n",
        "# #     break\n",
        "\n",
        "# epochs = 2\n",
        "# train_loss = 0\n",
        "# train_correct = 0\n",
        "# total = 0\n",
        "\n",
        "\n",
        "# for i in range(epochs):\n",
        "#     for index, (images, labels) in enumerate(train_dataloader):\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "#         outputs = model.forward(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimiser.step()\n",
        "\n",
        "#         train_loss += loss.item()\n",
        "\n",
        "#         print(torch.sum(torch.argmax(outputs, 1) == labels))\n",
        "\n",
        "        \n",
        "\n",
        "#         total += labels.size(0)\n",
        "\n",
        "#         # misc.progress_bar(index, len(train_dataloader), 'Loss: %.4f | Acc: %.3f%% (%d/%d)'\n",
        "#         #                  % (train_loss / (index + 1), 100. * train_correct / total, train_correct, total))\n",
        "\n",
        "#         optimiser.zero_grad()\n",
        "#         scheduler.step()\n",
        "#         curr_lr = scheduler.get_last_lr()\n",
        "\n",
        "#         print(\n",
        "#             f'Epoch: [{i+1} / {epochs}], Step [{index + 1} / {len(train_dataloader)}], Loss: {loss.item()}, lr: {curr_lr[0]}')\n",
        "\n",
        "\n",
        "# print(train_loss, train_correct / total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuS5JxmLW2VM"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tus44rm4-VSQ"
      },
      "outputs": [],
      "source": [
        "# model.features[0].weight.data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvVcv8WD-2kk"
      },
      "outputs": [],
      "source": [
        "# def plot_filters_single_channel_big(t):\n",
        "    \n",
        "#     #setting the rows and columns\n",
        "#     nrows = t.shape[0]*t.shape[2]\n",
        "#     ncols = t.shape[1]*t.shape[3]\n",
        "    \n",
        "    \n",
        "#     npimg = np.array(t.numpy(), np.float32)\n",
        "#     npimg = npimg.transpose((0, 2, 1, 3))\n",
        "#     npimg = npimg.ravel().reshape(nrows, ncols)\n",
        "    \n",
        "#     npimg = npimg.T\n",
        "    \n",
        "#     fig, ax = plt.subplots(figsize=(ncols/10, nrows/200))    \n",
        "#     imgplot = sns.heatmap(npimg, xticklabels=False, yticklabels=False, cmap='gray', ax=ax, cbar=False)\n",
        "\n",
        "# def plot_filters_multi_channel(t):\n",
        "    \n",
        "#     #get the number of kernals\n",
        "#     num_kernels = t.shape[0]    \n",
        "    \n",
        "#     #define number of columns for subplots\n",
        "#     num_cols = 12\n",
        "#     #rows = num of kernels\n",
        "#     num_rows = num_kernels\n",
        "    \n",
        "#     #set the figure size\n",
        "#     fig = plt.figure(figsize=(num_cols,num_rows))\n",
        "    \n",
        "#     #looping through all the kernels\n",
        "#     for i in range(t.shape[0]):\n",
        "#         ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
        "        \n",
        "#         #for each kernel, we convert the tensor to numpy \n",
        "#         npimg = np.array(t[i].numpy(), np.float32)\n",
        "#         #standardize the numpy image\n",
        "#         npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "#         npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "#         npimg = npimg.transpose((1, 2, 0))\n",
        "#         ax1.imshow(npimg)\n",
        "#         ax1.axis('off')\n",
        "#         ax1.set_title(str(i))\n",
        "#         ax1.set_xticklabels([])\n",
        "#         ax1.set_yticklabels([])\n",
        "        \n",
        "#     plt.savefig('myimage.png', dpi=100)    \n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# def plot_filters_single_channel(t):\n",
        "    \n",
        "#     #kernels depth * number of kernels\n",
        "#     nplots = t.shape[0]*t.shape[1]\n",
        "#     ncols = 12\n",
        "    \n",
        "#     nrows = 1 + nplots//ncols\n",
        "#     #convert tensor to numpy image\n",
        "#     npimg = np.array(t.numpy(), np.float32)\n",
        "    \n",
        "#     count = 0\n",
        "#     fig = plt.figure(figsize=(ncols, nrows))\n",
        "    \n",
        "#     #looping through all the kernels in each channel\n",
        "#     for i in range(t.shape[0]):\n",
        "#         for j in range(t.shape[1]):\n",
        "#             count += 1\n",
        "#             ax1 = fig.add_subplot(nrows, ncols, count)\n",
        "#             npimg = np.array(t[i, j].numpy(), np.float32)\n",
        "#             npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "#             npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "#             ax1.imshow(npimg)\n",
        "#             ax1.set_title(str(i) + ',' + str(j))\n",
        "#             ax1.axis('off')\n",
        "#             ax1.set_xticklabels([])\n",
        "#             ax1.set_yticklabels([])\n",
        "   \n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# def plot_weights(model, layer_num, single_channel = True, collated = False):\n",
        "  \n",
        "#   #extracting the model features at the particular layer number\n",
        "#   layer = model.features[layer_num]\n",
        "  \n",
        "#   #checking whether the layer is convolution layer or not \n",
        "#   if isinstance(layer, nn.Conv2d):\n",
        "#     #getting the weight tensor data\n",
        "#     weight_tensor = model.features[layer_num].weight.data\n",
        "    \n",
        "#     if single_channel:\n",
        "#       if collated:\n",
        "#         plot_filters_single_channel_big(weight_tensor)\n",
        "#       else:\n",
        "#         plot_filters_single_channel(weight_tensor)\n",
        "        \n",
        "#     else:\n",
        "#       if weight_tensor.shape[1] == 3:\n",
        "#         plot_filters_multi_channel(weight_tensor)\n",
        "#       else:\n",
        "#         print(\"Can only plot weights with three channels with single channel = False\")\n",
        "        \n",
        "#   else:\n",
        "#     print(\"Can only visualize layers which are convolutional\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQhPLTbm-6-C"
      },
      "outputs": [],
      "source": [
        "# plot_weights(alexnet.cpu(), 0, single_channel=False)\n",
        "# print(\"Plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGAw--iaMAAl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx4wSakWAmHa"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for index, (inputs, labels) in enumerate(train_dataloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                print(f\"[{index} / {len(train_dataloader)}], Loss: {loss}\")\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / len(train_dataloader)\n",
        "            epoch_acc = running_corrects.double() / len(train_dataloader)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSggkdvDK4IN"
      },
      "outputs": [],
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "d61f66d2119f44dbbd057ed12284ffb8",
            "3511cff75abe433fbd15b52aa1b03f20",
            "84749b33bc7e4b51af06b7dc9a5b8d86",
            "6290a82cef1046998b741ccd8304e17d",
            "bae69d67420f46298e3fb4d5877917d8",
            "e4df3c5324c5475a9936f51bb33a94fc",
            "5c0af09fc51c44e59f3d207ce3178be1",
            "608a58d44d0e4761b1c66bc0593bf6ed",
            "3241660761a44ef29c3cc219453f7a6e",
            "9e2188d9db984fccbcc65956f913537e",
            "e408e22dc922404f9d6d48edf2ce847c"
          ]
        },
        "id": "52Yz5kBDK66Q",
        "outputId": "cdf25a0f-83e8-4cf9-cbcb-c053671adebe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d61f66d2119f44dbbd057ed12284ffb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_ft = torchvision.models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 200)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QAk-QO1K8_u",
        "outputId": "a021fb8a-acc1-462c-bd9f-bed3d916fee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "[0 / 781], Loss: 5.390902042388916\n",
            "[1 / 781], Loss: 5.468130111694336\n",
            "[2 / 781], Loss: 5.578795433044434\n",
            "[3 / 781], Loss: 5.468331336975098\n",
            "[4 / 781], Loss: 5.609635829925537\n",
            "[5 / 781], Loss: 5.516209125518799\n",
            "[6 / 781], Loss: 5.485930919647217\n",
            "[7 / 781], Loss: 5.498056888580322\n",
            "[8 / 781], Loss: 5.546319484710693\n",
            "[9 / 781], Loss: 5.491790771484375\n",
            "[10 / 781], Loss: 5.600458145141602\n",
            "[11 / 781], Loss: 5.443974018096924\n",
            "[12 / 781], Loss: 5.45530891418457\n",
            "[13 / 781], Loss: 5.4255051612854\n",
            "[14 / 781], Loss: 5.495804786682129\n",
            "[15 / 781], Loss: 5.46973991394043\n",
            "[16 / 781], Loss: 5.4924845695495605\n",
            "[17 / 781], Loss: 5.454599380493164\n",
            "[18 / 781], Loss: 5.409163475036621\n",
            "[19 / 781], Loss: 5.379437446594238\n",
            "[20 / 781], Loss: 5.456066131591797\n",
            "[21 / 781], Loss: 5.496902942657471\n",
            "[22 / 781], Loss: 5.3322319984436035\n"
          ]
        }
      ],
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp3saD3cMRMr"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGMoI5WNiy2T"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TinyImageNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3241660761a44ef29c3cc219453f7a6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3511cff75abe433fbd15b52aa1b03f20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c0af09fc51c44e59f3d207ce3178be1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "608a58d44d0e4761b1c66bc0593bf6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6290a82cef1046998b741ccd8304e17d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3241660761a44ef29c3cc219453f7a6e",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_608a58d44d0e4761b1c66bc0593bf6ed",
            "value": 46830571
          }
        },
        "84749b33bc7e4b51af06b7dc9a5b8d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c0af09fc51c44e59f3d207ce3178be1",
            "placeholder": "​",
            "style": "IPY_MODEL_e4df3c5324c5475a9936f51bb33a94fc",
            "value": "100%"
          }
        },
        "9e2188d9db984fccbcc65956f913537e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bae69d67420f46298e3fb4d5877917d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e408e22dc922404f9d6d48edf2ce847c",
            "placeholder": "​",
            "style": "IPY_MODEL_9e2188d9db984fccbcc65956f913537e",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 95.9MB/s]"
          }
        },
        "d61f66d2119f44dbbd057ed12284ffb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84749b33bc7e4b51af06b7dc9a5b8d86",
              "IPY_MODEL_6290a82cef1046998b741ccd8304e17d",
              "IPY_MODEL_bae69d67420f46298e3fb4d5877917d8"
            ],
            "layout": "IPY_MODEL_3511cff75abe433fbd15b52aa1b03f20"
          }
        },
        "e408e22dc922404f9d6d48edf2ce847c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4df3c5324c5475a9936f51bb33a94fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
